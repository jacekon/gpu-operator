---
apiVersion: v1
kind: Namespace
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    app.kubernetes.io/managed-by: kyma-gpu-operator-module
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-operator
  namespace: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
rules:
  - apiGroups: ["*"]
    resources: ["*"]
    verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator
subjects:
  - kind: ServiceAccount
    name: gpu-operator
    namespace: gpu-operator
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-instructions
  namespace: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
data:
  README.md: |
    # GPU Operator Installation
    
    This ConfigMap contains instructions for installing the NVIDIA GPU Operator.
    
    ## Prerequisites
    
    1. Ensure you have a GPU worker pool configured in your Kyma cluster
    2. GPU nodes should have the appropriate taints/labels
    3. Helm 3.x must be available
    
    ## Installation Steps
    
    The GPU operator will be installed using Helm with the following command:
    
    ```bash
    # Add the NVIDIA Helm repository
    helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
    helm repo update
    
    # Install GPU Operator with Garden Linux optimized values
    helm upgrade --install --create-namespace \
      -n gpu-operator gpu-operator nvidia/gpu-operator \
      --values /module-data/gpu-operator-values.yaml
    ```
    
    ## Verification
    
    After installation, verify the GPU operator is running:
    
    ```bash
    kubectl get pods -n gpu-operator
    ```
    
    ## Testing GPU Access
    
    Deploy a test workload:
    
    ```bash
    kubectl apply -f - <<EOF
    apiVersion: v1
    kind: Pod
    metadata:
      name: gpu-test
    spec:
      containers:
      - name: gpu-test
        image: nvcr.io/nvidia/cuda:13.0.1-runtime-ubuntu24.04
        command: ["nvidia-smi"]
        resources:
          limits:
            nvidia.com/gpu: 1
      restartPolicy: Never
    EOF
    ```
    
    Check the logs:
    ```bash
    kubectl logs gpu-test
    ```
